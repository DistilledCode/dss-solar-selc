{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from random import shuffle\n",
    "import spacy\n",
    "import msgpack\n",
    "from dss_selc.utils import PRJ_PATH\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "with open(PRJ_PATH / \"dss-selc-dump/glove/corpus.txt\", \"r\") as f:\n",
    "    corpus = f.readlines()\n",
    "print(len(corpus), \"docs loaded\")\n",
    "\n",
    "\n",
    "def dump_corpus(new_corpus):\n",
    "    fp = PRJ_PATH / \"dss-selc-dump/glove/processed_corpus.msgpack\"\n",
    "    with open(fp, \"wb\") as f:\n",
    "        msgpack.dump(new_corpus, f)\n",
    "\n",
    "\n",
    "new_corpus = []\n",
    "n_ = len(corpus)\n",
    "for indx, doc_ in enumerate(corpus, start=1):\n",
    "    new_doc = []\n",
    "    doc = nlp(doc_)\n",
    "    print(f\"\\r[*] Processing: {indx:>07}/{n_:>07}, \", end=\"\")\n",
    "    for sentence in doc.sents:\n",
    "        for token in sentence:\n",
    "            if token.is_punct is True or token.is_stop is True:\n",
    "                continue\n",
    "            new_doc.append(str(token.lemma_)lower())\n",
    "    print(f\"added {len(new_doc):>03} tokens.\", \" \" * 20, end=\"\")\n",
    "    new_corpus.append(new_doc)\n",
    "    if indx % 10 == 0:\n",
    "        dump_corpus(new_corpus)\n",
    "        \n",
    "        \n",
    "shuffle(new_corpus)\n",
    "\n",
    "dump_corpus(new_corpus)\n",
    "\n",
    "\n",
    "\n",
    "glove_vectors = KeyedVectors.load_word2vec_format(\n",
    "    fname=\"../.models/glove/glove.840B.300d.txt\",\n",
    "    binary=False,\n",
    "    no_header=True,\n",
    ")\n",
    "\n",
    "#! creating base model class. Vector size should match that of pretrained vectors\n",
    "base_model = Word2Vec(\n",
    "    vector_size=300,\n",
    "    min_count=5,\n",
    "    epochs=40,\n",
    "    workers=12,\n",
    ")\n",
    "base_model.build_vocab(new_corpus)\n",
    "total_examples = base_model.corpus_count\n",
    "\n",
    "#! add pretrained GloVe's vocabulary\n",
    "base_model.build_vocab(list(glove_vectors.key_to_index.keys()), update=True)\n",
    "\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "\n",
    "class callback(CallbackAny2Vec):\n",
    "    \"\"\"Callback to print loss after each epoch.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.loss_to_be_subed = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss_now = loss - self.loss_to_be_subed\n",
    "        self.loss_to_be_subed = loss\n",
    "        print(f\"Loss after epoch {self.epoch:>02}: {loss_now:>010}\")\n",
    "        self.epoch += 1\n",
    "\n",
    "\n",
    "#! Training the model\n",
    "base_model.train(\n",
    "    new_corpus,\n",
    "    total_examples=total_examples,\n",
    "    epochs=base_model.epochs,\n",
    "    compute_loss=True,\n",
    "    callbacks=[callback()],\n",
    ")\n",
    "base_model_wv = base_model.wv\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "dr = PRJ_PATH / \"dss-selc-dump/glove/model_01/\"\n",
    "dr.mkdir(exist_ok=True, parents=True)\n",
    "base_model.save(str(dr) + \"/base_model.model\")\n",
    "\n",
    "# Find top 10 most similar words to \"example\"\n",
    "similar_words = base_model_wv.most_similar(\"solar\", topn=20)\n",
    "\n",
    "# Print the similar words and their cosine similarities\n",
    "for word, score in similar_words:\n",
    "    # print(f\"{repr(word)}: {score}\")\n",
    "    print(f\"{word}: {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
